{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import unicodedata\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/NepaliTaggedCorpus/new_submissions_parallel_corpus_project_Nepal/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbfile = open(path+'/'+'processed_tag', 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pickle.load(dbfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[६१,  वर्षीय,  पियरे,  भिन्केन,  नोभेम्बर,  २९...</td>\n",
       "      <td>[&lt;CD&gt;, &lt;JJ&gt;, &lt;NNP&gt;, &lt;NNP&gt;, &lt;NNP&gt;, &lt;CD&gt;, &lt;POP&gt;,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[श्री,  भिन्केन,  डच,  प्रकाशन,  समूह,  एल्सेभ...</td>\n",
       "      <td>[&lt;NN&gt;, &lt;NNP&gt;, &lt;NNP&gt;, &lt;NN&gt;, &lt;NN&gt;, &lt;NNP&gt;, &lt;FB&gt;, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[कन्सोलिडेटिड,  गोल्ड,  फिल्ड्स,  पीएलसी, का, ...</td>\n",
       "      <td>[&lt;NNP&gt;, &lt;NN&gt;, &lt;NN&gt;, &lt;NNP&gt;, &lt;PKO&gt;, &lt;JJ&gt;, &lt;NN&gt;, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[एकताका,  केन्ट,  चुरोट, को,  फिल्टर,  बनाउन, ...</td>\n",
       "      <td>[&lt;RBO&gt;, &lt;NNP&gt;, &lt;NN&gt;, &lt;PKO&gt;, &lt;NN&gt;, &lt;VBI&gt;, &lt;NN&gt;,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[यस, सँग, को,  छोटो,  सम्पर्क, बाट,  मात्र,  प...</td>\n",
       "      <td>[&lt;DUM&gt;, &lt;POP&gt;, &lt;PKO&gt;, &lt;JJM&gt;, &lt;NN&gt;, &lt;POP&gt;, &lt;RP&gt;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  [६१,  वर्षीय,  पियरे,  भिन्केन,  नोभेम्बर,  २९...   \n",
       "1  [श्री,  भिन्केन,  डच,  प्रकाशन,  समूह,  एल्सेभ...   \n",
       "2  [कन्सोलिडेटिड,  गोल्ड,  फिल्ड्स,  पीएलसी, का, ...   \n",
       "3  [एकताका,  केन्ट,  चुरोट, को,  फिल्टर,  बनाउन, ...   \n",
       "4  [यस, सँग, को,  छोटो,  सम्पर्क, बाट,  मात्र,  प...   \n",
       "\n",
       "                                                 tag  \n",
       "0  [<CD>, <JJ>, <NNP>, <NNP>, <NNP>, <CD>, <POP>,...  \n",
       "1  [<NN>, <NNP>, <NNP>, <NN>, <NN>, <NNP>, <FB>, ...  \n",
       "2  [<NNP>, <NN>, <NN>, <NNP>, <PKO>, <JJ>, <NN>, ...  \n",
       "3  [<RBO>, <NNP>, <NN>, <PKO>, <NN>, <VBI>, <NN>,...  \n",
       "4  [<DUM>, <POP>, <PKO>, <JJM>, <NN>, <POP>, <RP>...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['len_txt'] =data_df['text'].map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['len_tag'] =data_df['tag'].map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "      <th>len_txt</th>\n",
       "      <th>len_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [text, tag, len_txt, len_tag]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[data_df['len_txt']!=data_df['len_tag']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hence from the operation, we figure out that there is not mismatch between tags and text tokens "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "      <th>len_txt</th>\n",
       "      <th>len_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[६१,  वर्षीय,  पियरे,  भिन्केन,  नोभेम्बर,  २९...</td>\n",
       "      <td>[&lt;CD&gt;, &lt;JJ&gt;, &lt;NNP&gt;, &lt;NNP&gt;, &lt;NNP&gt;, &lt;CD&gt;, &lt;POP&gt;,...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[श्री,  भिन्केन,  डच,  प्रकाशन,  समूह,  एल्सेभ...</td>\n",
       "      <td>[&lt;NN&gt;, &lt;NNP&gt;, &lt;NNP&gt;, &lt;NN&gt;, &lt;NN&gt;, &lt;NNP&gt;, &lt;FB&gt;, ...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[कन्सोलिडेटिड,  गोल्ड,  फिल्ड्स,  पीएलसी, का, ...</td>\n",
       "      <td>[&lt;NNP&gt;, &lt;NN&gt;, &lt;NN&gt;, &lt;NNP&gt;, &lt;PKO&gt;, &lt;JJ&gt;, &lt;NN&gt;, ...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[एकताका,  केन्ट,  चुरोट, को,  फिल्टर,  बनाउन, ...</td>\n",
       "      <td>[&lt;RBO&gt;, &lt;NNP&gt;, &lt;NN&gt;, &lt;PKO&gt;, &lt;NN&gt;, &lt;VBI&gt;, &lt;NN&gt;,...</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[यस, सँग, को,  छोटो,  सम्पर्क, बाट,  मात्र,  प...</td>\n",
       "      <td>[&lt;DUM&gt;, &lt;POP&gt;, &lt;PKO&gt;, &lt;JJM&gt;, &lt;NN&gt;, &lt;POP&gt;, &lt;RP&gt;...</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  [६१,  वर्षीय,  पियरे,  भिन्केन,  नोभेम्बर,  २९...   \n",
       "1  [श्री,  भिन्केन,  डच,  प्रकाशन,  समूह,  एल्सेभ...   \n",
       "2  [कन्सोलिडेटिड,  गोल्ड,  फिल्ड्स,  पीएलसी, का, ...   \n",
       "3  [एकताका,  केन्ट,  चुरोट, को,  फिल्टर,  बनाउन, ...   \n",
       "4  [यस, सँग, को,  छोटो,  सम्पर्क, बाट,  मात्र,  प...   \n",
       "\n",
       "                                                 tag  len_txt  len_tag  \n",
       "0  [<CD>, <JJ>, <NNP>, <NNP>, <NNP>, <CD>, <POP>,...       16       16  \n",
       "1  [<NN>, <NNP>, <NNP>, <NN>, <NN>, <NNP>, <FB>, ...       11       11  \n",
       "2  [<NNP>, <NN>, <NN>, <NNP>, <PKO>, <JJ>, <NN>, ...       25       25  \n",
       "3  [<RBO>, <NNP>, <NN>, <PKO>, <NN>, <VBI>, <NN>,...       43       43  \n",
       "4  [<DUM>, <POP>, <PKO>, <JJM>, <NN>, <POP>, <RP>...       38       38  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['text'] = data_df['text'].apply(lambda x : [\"\".join(item.split(\" \")) for item in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Unicode value into ASCII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['ascii_text'] = data_df['text'].apply(lambda x : [unicode_to_ascii(item) for item in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.drop(columns=['text','len_txt','len_tag'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df[['ascii_text','tag']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ascii_text</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[६१, वरषीय, पियर, भिनकन, नोभमबर, २९, बाट, सलला...</td>\n",
       "      <td>[&lt;CD&gt;, &lt;JJ&gt;, &lt;NNP&gt;, &lt;NNP&gt;, &lt;NNP&gt;, &lt;CD&gt;, &lt;POP&gt;,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[शरी, भिनकन, डच, परकाशन, समह, एलसभियर, एन.भी.,...</td>\n",
       "      <td>[&lt;NN&gt;, &lt;NNP&gt;, &lt;NNP&gt;, &lt;NN&gt;, &lt;NN&gt;, &lt;NNP&gt;, &lt;FB&gt;, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[कनसोलिडटिड, गोलड, फिलडस, पीएलसी, का, परव, सभा...</td>\n",
       "      <td>[&lt;NNP&gt;, &lt;NN&gt;, &lt;NN&gt;, &lt;NNP&gt;, &lt;PKO&gt;, &lt;JJ&gt;, &lt;NN&gt;, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[एकताका, कनट, चरोट, को, फिलटर, बनाउन, परयोग, भ...</td>\n",
       "      <td>[&lt;RBO&gt;, &lt;NNP&gt;, &lt;NN&gt;, &lt;PKO&gt;, &lt;NN&gt;, &lt;VBI&gt;, &lt;NN&gt;,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[यस, सग, को, छोटो, समपरक, बाट, मातर, पनि, दशकौ...</td>\n",
       "      <td>[&lt;DUM&gt;, &lt;POP&gt;, &lt;PKO&gt;, &lt;JJM&gt;, &lt;NN&gt;, &lt;POP&gt;, &lt;RP&gt;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          ascii_text  \\\n",
       "0  [६१, वरषीय, पियर, भिनकन, नोभमबर, २९, बाट, सलला...   \n",
       "1  [शरी, भिनकन, डच, परकाशन, समह, एलसभियर, एन.भी.,...   \n",
       "2  [कनसोलिडटिड, गोलड, फिलडस, पीएलसी, का, परव, सभा...   \n",
       "3  [एकताका, कनट, चरोट, को, फिलटर, बनाउन, परयोग, भ...   \n",
       "4  [यस, सग, को, छोटो, समपरक, बाट, मातर, पनि, दशकौ...   \n",
       "\n",
       "                                                 tag  \n",
       "0  [<CD>, <JJ>, <NNP>, <NNP>, <NNP>, <CD>, <POP>,...  \n",
       "1  [<NN>, <NNP>, <NNP>, <NN>, <NN>, <NNP>, <FB>, ...  \n",
       "2  [<NNP>, <NN>, <NN>, <NNP>, <PKO>, <JJ>, <NN>, ...  \n",
       "3  [<RBO>, <NNP>, <NN>, <PKO>, <NN>, <VBI>, <NN>,...  \n",
       "4  [<DUM>, <POP>, <PKO>, <JJM>, <NN>, <POP>, <RP>...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding Sequence for batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec and vec2word conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dictonary = list(set(sum(data_df['ascii_text'].tolist(),[])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'अपचछदन',\n",
       " 'कोजनरसन-पलानट',\n",
       " 'असवीकार',\n",
       " 'पयालइसटाइनी',\n",
       " 'निषकासित',\n",
       " 'पहिचान',\n",
       " 'एभन']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dictonary[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_dictionary = list(set(sum(data_df['tag'].tolist(),[])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<RP>', '<DM>', '<CS>', '<FB>', '<CC>', '<QW>', '<VBF>', '<JJ>']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_dictionary[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "int2word = dict(enumerate(word_dictonary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2int = {int2word[idx]:idx for idx in int2word.keys() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "int2tag = dict(enumerate(tag_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2int = {int2tag[idx]:idx for idx in int2tag.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_RATIO = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_num = round((1-SPLIT_RATIO)*data_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data_df[:split_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = data_df[split_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3434, 2), (858, 2))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.shape,valid_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POS_Tagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, target_size):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, target_size)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = self.log_softmax(tag_space)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(word_dictonary)\n",
    "OUTPUT_DIM = len(tag_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11993, 39)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_DIM,OUTPUT_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_model = POS_Tagger(EMBEDDING_DIM, HIDDEN_DIM, INPUT_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "POS_Tagger(\n",
       "  (word_embeddings): Embedding(11993, 300)\n",
       "  (lstm): LSTM(300, 150)\n",
       "  (hidden2tag): Linear(in_features=150, out_features=39, bias=True)\n",
       "  (log_softmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initailization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss, Optimizer\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(sample_model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = list(train_dataset.itertuples(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = list(valid_dataset.itertuples(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train_dataset[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_sample = valid_dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_int = prepare_sequence(sample[0][0], word2int)\n",
    "tag_int = prepare_sequence(sample[0][1], tag2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = sample_model(sent_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(score,tag_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6500182151794434"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    log_ps = sample_model(sent_int)\n",
    "    test_loss = criterion(log_ps, tag_int).item()\n",
    "    ps = torch.exp(log_ps)\n",
    "    top_p, top_class = ps.topk(1, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([38,  7, 11, 11, 11, 38, 20, 19, 24, 19, 20, 19, 19, 20,  8, 18])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[38],\n",
       "        [ 7],\n",
       "        [11],\n",
       "        [11],\n",
       "        [11],\n",
       "        [38],\n",
       "        [20],\n",
       "        [19],\n",
       "        [24],\n",
       "        [19],\n",
       "        [20],\n",
       "        [19],\n",
       "        [19],\n",
       "        [20],\n",
       "        [ 8],\n",
       "        [18]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EPOCHS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-b0a93148feeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepoch_loss_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mepoch_loss_valid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mnet_loss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'EPOCHS' is not defined"
     ]
    }
   ],
   "source": [
    "epoch_loss_train=[]\n",
    "epoch_loss_valid=[]\n",
    "for epoch in range(EPOCHS):\n",
    "    net_loss_train = 0\n",
    "    for sentence, tags in sample:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        sent_int = prepare_sequence(sentence, word2int)\n",
    "        tag_int = prepare_sequence(tags, tag2int)\n",
    "        \n",
    "\n",
    "        #Step 3. Run our forward pass.\n",
    "        try:\n",
    "            tag_scores = model(sent_int)\n",
    "            break\n",
    "            # Step 4. Compute the loss, gradients, and update the parameters \n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(tag_scores, tag_int)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            net_loss_train+=loss.item()\n",
    "            \n",
    "        except:\n",
    "            continue\n",
    "    # cross validation\n",
    "    with torch.no_grad():\n",
    "        net_loss_valid=0\n",
    "        accuracy=0\n",
    "        for sentence,tags in valid_sample:\n",
    "            sent_int = prepare_sequence(sentence,word2int)\n",
    "            tag_int = prepare_sequence(tags,tag2int)\n",
    "            #Step 3. Run our forward pass.\n",
    "            try:\n",
    "                tag_scores = model(sent_int)\n",
    "\n",
    "                # Step 4. Compute the loss \n",
    "                net_loss_valid+=criterion(tag_scores, tag_int)\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "    epoch_loss_train.append(net_loss_train/len(train_dataset))\n",
    "    epoch_loss_valid.append(net_loss_valid/len(valid_dataset))\n",
    "#     torch.save(model.state_dict(),'../model/POS_Tagger_model_epoch'+str(epoch))\n",
    "    if epoch%1 == 0:\n",
    "        print('Epoch: {}/{}.............'.format(epoch+1, EPOCHS), end=' ')\n",
    "        print(\"Training Loss: {:.2f}\".format(net_loss_train/len(train_dataset)),end=' ')\n",
    "        print(\"Validation Loss: {:.2f}\".format(net_loss_valid/len(valid_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch_loss_train=[]\n",
    "# epoch_loss_valid=[]\n",
    "# for epoch in range(EPOCHS):\n",
    "#     net_loss_train = 0\n",
    "#     for sentence, tags in train_dataset:\n",
    "#         # Step 1. Remember that Pytorch accumulates gradients.\n",
    "#         # We need to clear them out before each instance\n",
    "#         model.zero_grad()\n",
    "#         # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "#         sent_int = prepare_sequence(sentence, word2int)\n",
    "#         tag_int = prepare_sequence(tags, tag2int)\n",
    "        \n",
    "\n",
    "#         #Step 3. Run our forward pass.\n",
    "#         try:\n",
    "#             tag_scores = model(sent_int)\n",
    "\n",
    "#             # Step 4. Compute the loss, gradients, and update the parameters \n",
    "#             loss = criterion(tag_scores, tag_int)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             net_loss_train+=loss.item()\n",
    "            \n",
    "#         except:\n",
    "#             continue\n",
    "#     # cross validation\n",
    "#     with torch.no_grad():\n",
    "#         net_loss_valid=0\n",
    "#         accuracy=0\n",
    "#         for sentence,tags in valid_dataset:\n",
    "#             sent_int = prepare_sequence(sentence,word2int)\n",
    "#             tag_int = prepare_sequence(tags,tag2int)\n",
    "#             #Step 3. Run our forward pass.\n",
    "#             try:\n",
    "#                 tag_scores = model(sent_int)\n",
    "\n",
    "#                 # Step 4. Compute the loss \n",
    "#                 net_loss_valid+=criterion(tag_scores, tag_int)\n",
    "#             except:\n",
    "#                 continue\n",
    "                \n",
    "#     epoch_loss_train.append(net_loss_train/len(train_dataset))\n",
    "#     epoch_loss_valid.append(net_loss_valid/len(valid_dataset))\n",
    "#     torch.save(model.state_dict(),'../model/POS_Tagger_model_epoch'+str(epoch))\n",
    "#     if epoch%5 == 0:\n",
    "#         print('Epoch: {}/{}.............'.format(epoch+1, EPOCHS), end=' ')\n",
    "#         print(\"Training Loss: {:.2f}\".format(net_loss_train/len(train_dataset)),end=' ')\n",
    "#         print(\"Validation Loss: {:.2f}\".format(net_loss_valid/len(valid_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_loss_valid = torch.FloatTensor(epoch_loss_valid).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_loss_train)\n",
    "plt.plot(epoch_loss_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    print(valid_dataset[0][0])\n",
    "    print('True value')\n",
    "    print('---------------------------------------')\n",
    "    print(valid_dataset[0][1])\n",
    "    score = model(prepare_sequence(valid_dataset[0][0], word2int))\n",
    "    print('----------------------------------------')\n",
    "    value=[]\n",
    "    for item in score:\n",
    "        value.append(torch.argmax(item))\n",
    "    value = torch.IntTensor(value).tolist()\n",
    "    tag=[]\n",
    "    for item in value:\n",
    "            tag.append(int2tag[item])\n",
    "    print('predicted_value')        \n",
    "    print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    sent = ['म','खाना','घर']\n",
    "    aasa = prepare_sequence(sent, word2int)\n",
    "    \n",
    "    score = model(aasa)\n",
    "    print('----------------------------------------')\n",
    "    value=[]\n",
    "    for item in score:\n",
    "        value.append(torch.argmax(item))\n",
    "    value = torch.IntTensor(value).tolist()\n",
    "    tag=[]\n",
    "    for item in value:\n",
    "            tag.append(int2tag[item])\n",
    "    print('predicted_value')        \n",
    "    print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
